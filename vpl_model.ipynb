{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "class SensoryNeurons(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_neurons, in_dim = 1, plastic = True, params = (5, -2.5)):\n",
    "        super(SensoryNeurons, self).__init__()\n",
    "        \n",
    "        self.in_dim = in_dim\n",
    "        self.plastic = plastic\n",
    "        self.params = params\n",
    "        self.num_neurons = num_neurons\n",
    "\n",
    "        self.linear = nn.Linear(self.in_dim,self.num_neurons, bias = True)\n",
    "        self.resp_func = nn.Sigmoid()\n",
    "\n",
    "        if not self.plastic:\n",
    "            self.set_linear_weights()\n",
    "            self.linear.weight.requires_grad = False\n",
    "        \n",
    "    def set_linear_weights(self):\n",
    "        \n",
    "        self.linear.weight = torch.nn.Parameter(data = self.params[0] + 0.2*torch.randn(self.num_neurons,1), requires_grad = False)\n",
    "        self.linear.bias = torch.nn.Parameter(data = self.params[1] + 0.2*torch.randn(self.num_neurons), requires_grad = False)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.linear(x)\n",
    "        x = self.resp_func(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "        \n",
    "        \n",
    "class SensoryPopulation(nn.Module):\n",
    "    def __init__(self, num_neurons, plastic = True, population_ratio = 0.5):\n",
    "        super(SensoryPopulation, self).__init__()\n",
    "        \n",
    "        self.num_neurons = num_neurons\n",
    "        self.plastic = plastic\n",
    "        self.num_neurons_group1 = round(self.num_neurons * population_ratio)\n",
    "        self.num_neurons_group2 = round(self.num_neurons * (1.0 - population_ratio))\n",
    "        self.sensory_neurons_1 = SensoryNeurons(num_neurons = self.num_neurons_group1, in_dim = 1, plastic = self.plastic, params = (5, -2.5))\n",
    "        self.sensory_neurons_2 = SensoryNeurons(num_neurons = self.num_neurons_group2, in_dim = 1, plastic = self.plastic, params = (-5, -2.5))\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        x1 = self.sensory_neurons_1(x)\n",
    "        x2 = self.sensory_neurons_2(x)\n",
    "        \n",
    "        out = torch.cat((x1, x2),dim = 1)       \n",
    "        \n",
    "        return out\n",
    "            \n",
    "        \n",
    "    \n",
    "class Readout(nn.Module):\n",
    "    def __init__(self,num_classes = 2, in_dim = 10, readout_plastic = False):\n",
    "        super(Readout, self).__init__()\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        self.in_dim = in_dim\n",
    "        self.readout_plastic = readout_plastic\n",
    "        \n",
    "        self.readout_layer = nn.Linear(self.in_dim, self.num_classes, bias = True)\n",
    "        self._set_weights()\n",
    "        \n",
    "    def _set_weights(self):\n",
    "        self.readout_layer.bias = nn.Parameter(1*torch.ones(1),requires_grad=True) #\n",
    "        self.readout_layer.weight = nn.Parameter(torch.ones(self.readout_layer.weight.shape)/(self.readout_layer.weight.shape[0]+self.readout_layer.weight.shape[1]),\n",
    "                                                 requires_grad=self.readout_plastic) #\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        return (self.readout_layer(x))\n",
    "    \n",
    "    \n",
    "class Sensorimotor(nn.Module):\n",
    "    def __init__(self, num_sensory_neurons = 10, sensory_plastic = True, readout_plastic = False, sensory_pop_ratio = 0.5, num_classes = 2):\n",
    "        super(Sensorimotor, self).__init__()\n",
    "        \n",
    "        self.sensory_pop = SensoryPopulation(num_neurons = num_sensory_neurons, plastic = sensory_plastic, population_ratio = sensory_pop_ratio)\n",
    "        self.readout = Readout(num_classes = num_classes, in_dim = num_sensory_neurons, readout_plastic = readout_plastic)\n",
    "        self.sig = nn.Sigmoid()\n",
    "#         self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        sensory_out = self.sensory_pop(x)\n",
    "        readout_out = self.readout(sensory_out)\n",
    "#         print(readout_out)\n",
    "        y = self.sig(readout_out)\n",
    "#         y = self.softmax(readout_out)\n",
    "#         y = readout_out\n",
    "        \n",
    "        return y, readout_out\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "\n",
    "class Stimulus(data.DataLoader):\n",
    "    def __init__(self, min_coherence = 0.8, max_coherence = 1):\n",
    "        \n",
    "        self.min_coherence = min_coherence\n",
    "        self.max_coherence = max_coherence\n",
    "        \n",
    "        self.NUM_SAMPLPES_PER_CATEGORY = 1000\n",
    "        \n",
    "        data1 = torch.rand(self.NUM_SAMPLPES_PER_CATEGORY)*(self.max_coherence - self.min_coherence) + self.min_coherence\n",
    "        data2 = -(torch.rand(self.NUM_SAMPLPES_PER_CATEGORY)*(self.max_coherence - self.min_coherence) + self.min_coherence)\n",
    "#         target1 = torch.zeros(data1.shape, dtype = int)\n",
    "#         target2 = torch.ones(data2.shape, dtype = int)\n",
    "        target1 = torch.zeros(data1.shape)\n",
    "        target2 = torch.ones(data2.shape)\n",
    "        \n",
    "        self.data = torch.cat((data1, data2), dim = 0).unsqueeze(0).t()\n",
    "        self.target = torch.cat((target1, target2), dim = 0).unsqueeze(0).t()\n",
    " \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        \n",
    "        return (self.data[index], self.target[index])\n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.data)\n",
    "    \n",
    "     \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class ConfusionMeter(object):\n",
    "    '''compute and show confusion matrix'''\n",
    "    def __init__(self, num_class):\n",
    "        self.num_class = num_class\n",
    "        self.mat = np.zeros((num_class, num_class))\n",
    "        self.precision = []\n",
    "        self.recall = []\n",
    "\n",
    "    def update(self, pred, tar):\n",
    "        pred, tar = pred.cpu().numpy(), tar.cpu().numpy()\n",
    "        pred = np.squeeze(pred)\n",
    "        tar = np.squeeze(tar)\n",
    "        for p,t in zip(pred.flat, tar.flat):\n",
    "            self.mat[p][t] += 1\n",
    "\n",
    "    def print_mat(self):\n",
    "        print('Confusion Matrix: (target in columns)')\n",
    "        print(self.mat)\n",
    "\n",
    "    def plot_mat(self, path, dictionary=None, annotate=False):\n",
    "        plt.figure(dpi=600)\n",
    "        plt.imshow(self.mat,\n",
    "            cmap=plt.cm.jet,\n",
    "            interpolation=None,\n",
    "            extent=(0.5, np.shape(self.mat)[0]+0.5, np.shape(self.mat)[1]+0.5, 0.5))\n",
    "        width, height = self.mat.shape\n",
    "        if annotate:\n",
    "            for x in range(width):\n",
    "                for y in range(height):\n",
    "                    plt.annotate(str(int(self.mat[x][y])), xy=(y+1, x+1),\n",
    "                                 horizontalalignment='center',\n",
    "                                 verticalalignment='center',\n",
    "                                 fontsize=8)\n",
    "\n",
    "        if dictionary is not None:\n",
    "            plt.xticks([i+1 for i in range(width)],\n",
    "                       [dictionary[i] for i in range(width)],\n",
    "                       rotation='vertical')\n",
    "            plt.yticks([i+1 for i in range(height)],\n",
    "                       [dictionary[i] for i in range(height)])\n",
    "        plt.xlabel('Ground Truth')\n",
    "        plt.ylabel('Prediction')\n",
    "        plt.colorbar()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(path, format='svg')\n",
    "        plt.clf()\n",
    "\n",
    "        # for i in range(width):\n",
    "        #     if np.sum(self.mat[i,:]) != 0:\n",
    "        #         self.precision.append(self.mat[i,i] / np.sum(self.mat[i,:]))\n",
    "        #     if np.sum(self.mat[:,i]) != 0:\n",
    "        #         self.recall.append(self.mat[i,i] / np.sum(self.mat[:,i]))\n",
    "        # print('Average Precision: %0.4f' % np.mean(self.precision))\n",
    "        # print('Average Recall: %0.4f' % np.mean(self.recall))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def main(num_epochs = 1000, lr = 1e-1, batch_size = 100, learning_rule = 'backprop', model = None): # learning_rule can be 'backprop' or 'global_gain'\n",
    "    \n",
    "    torch.manual_seed(0)\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    if model is None:\n",
    "        model = Sensorimotor(num_sensory_neurons = 100, \n",
    "                             sensory_plastic = False, \n",
    "                             readout_plastic = False,\n",
    "                             sensory_pop_ratio = 0.8,\n",
    "                             num_classes = 1)\n",
    "    \n",
    "    print(model)\n",
    "    print(model.readout.readout_layer.weight)\n",
    "    print('\\n===========Check Grad============')\n",
    "    for name, param in model.named_parameters():\n",
    "        print(name, param.requires_grad)\n",
    "    print('=================================\\n')    \n",
    "    \n",
    "    params = model.parameters()\n",
    "    \n",
    "    \n",
    "    optimizer = optim.SGD(params, lr=lr, momentum=0, dampening=0, weight_decay=0, nesterov=False)\n",
    "#     loss = nn.CrossEntropyLoss()\n",
    "#     loss = nn.NLLLoss()\n",
    "#     loss = nn.MSELoss()\n",
    "    loss = nn.BCELoss(reduction='mean')\n",
    "    \n",
    "    dataset_train = Stimulus(min_coherence = .8, max_coherence = 1) # set the range of coherences\n",
    "    dataset_valid = Stimulus(min_coherence = .8, max_coherence = 1)\n",
    "#     dataset_valid = Stimulus(min_coherence = 0.04, max_coherence = 0.06)\n",
    "\n",
    "\n",
    "    sampler_train = data.RandomSampler(dataset_train)\n",
    "    train_dl = data.DataLoader(dataset_train,\n",
    "                             batch_size=batch_size,\n",
    "                             sampler=sampler_train,\n",
    "                             shuffle=False,\n",
    "                             num_workers=0,\n",
    "                             pin_memory=True,\n",
    "                             drop_last=True)\n",
    "    \n",
    "    sampler_valid = data.RandomSampler(dataset_valid)\n",
    "    valid_dl = data.DataLoader(dataset_valid,\n",
    "                             batch_size=batch_size,\n",
    "                             sampler=sampler_valid,\n",
    "                             shuffle=False,\n",
    "                             num_workers=0,\n",
    "                             pin_memory=True,\n",
    "                             drop_last=True)\n",
    "    \n",
    "\n",
    "\n",
    "    all_loss = []\n",
    "    all_loss_valid = []\n",
    "    print(f'bias = {model.readout.readout_layer.bias}')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        Loss = 0\n",
    "        \n",
    "        for stimulus, target in train_dl:\n",
    "            \n",
    "            decision, _ = model(stimulus)\n",
    "            L = loss(decision, target.squeeze())\n",
    "#             print(f'dec={decision}, t={target.squeeze()}, loss={L}')\n",
    "            \n",
    "            Loss += L/len(train_dl)\n",
    "#             print(f'bias = {model.readout.readout_layer.bias}', f'loss = {Loss}')\n",
    "            optimizer.zero_grad()\n",
    "            L.backward()      \n",
    "            \n",
    "            if learning_rule != 'backprop':\n",
    "                model = personalized_backward(model, stimulus, target, method=learning_rule)\n",
    "\n",
    "            \n",
    "            optimizer.step()\n",
    "            del L\n",
    "            \n",
    "            \n",
    "        Loss_valid = 0\n",
    "        conf_mat = ConfusionMeter(num_class=2)\n",
    "        i = 0\n",
    "        y1 = []\n",
    "        y0 = []\n",
    "        for stimulus, target in valid_dl:\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                decision, readout = model(stimulus)\n",
    "                \n",
    "                pred = decision.clone()\n",
    "                \n",
    "                decision_copy = decision.clone()\n",
    "                pred = target.clone()\n",
    "#                 y1.extend(readout[:,1].detach())\n",
    "#                 y0.extend(readout[:,0].detach())\n",
    "#                 idx1 = decision_copy[:,1] < decision_copy[:,0]\n",
    "#                 idx2 = decision_copy[:,1] >= decision_copy[:,0]\n",
    "                idx1 = decision_copy < 0.5\n",
    "                idx2 = decision_copy >= 0.5\n",
    "                pred[idx1] = 0\n",
    "                pred[idx2] = 1\n",
    "\n",
    "                target_copy = target.squeeze().int()\n",
    "                conf_mat.update(pred.int(),target_copy)\n",
    "                del pred\n",
    "                L = loss(decision, target.squeeze())\n",
    "                Loss_valid += L/len(train_dl)\n",
    "                \n",
    "#                 \n",
    "\n",
    "                del L\n",
    "        if epoch == 0:\n",
    "            conf_mat.print_mat()\n",
    "            \n",
    "        all_loss.append(Loss)\n",
    "        all_loss_valid.append(Loss_valid)\n",
    "        \n",
    "    conf_mat.print_mat()\n",
    "    print(model.readout.readout_layer.weight)\n",
    "    print(f'bias = {model.readout.readout_layer.bias}')\n",
    "\n",
    "\n",
    "        # print(f'epoch {epoch},   training Loss = {Loss},   validation Loss = {Loss_valid}')\n",
    "    return all_loss, all_loss_valid, model, y0, y1\n",
    "            \n",
    "            \n",
    "def personalized_backward(model, stimulus, target, method = 'global_gain'):\n",
    "    \n",
    "    if method == 'global_gain': \n",
    "        # only adds to the weight value:  if weight is negative (positive) adds a negative (positive) value \n",
    "        for n,p in model.named_parameters():\n",
    "\n",
    "            if p.requires_grad:\n",
    "#                 print(f'{n}, mean = {p.mean()}, std = {p.std()}')\n",
    "                p_sign = p.clone()\n",
    "                p_sign[p_sign>0] = 1\n",
    "                p_sign[p_sign<0] = -1\n",
    "                p.grad = -1*torch.ones((p.grad.shape),requires_grad=True)*p_sign\n",
    "    \n",
    "    if method == 'hebbian': \n",
    "        \n",
    "        for n,p in model.named_parameters():\n",
    "            \n",
    "            if ('readout' in n) and ('weight' in n):\n",
    "                output = model(stimulus)\n",
    "                s_out = model.sensory_pop(stimulus)\n",
    "                hebb_factor = torch.matmul(output.transpose(1,0),s_out)/s_out.shape[0]\n",
    "                p.grad = hebb_factor\n",
    "        \n",
    "    if method == 'hebbian_wfb':\n",
    "        \n",
    "        for n,p in model.named_parameters():\n",
    "            \n",
    "            if ('readout' in n) and ('weight' in n):\n",
    "                target = target.unsqueeze(0)\n",
    "                \n",
    "                # create feedback based on the target values\n",
    "                fb = torch.cat((target,(target-1).abs()),dim=0).squeeze().transpose(1,0)\n",
    "                \n",
    "                output = model(stimulus)\n",
    "                fb_modulated_output = output*fb\n",
    "                s_out = model.sensory_pop(stimulus)\n",
    "                hebb_factor = torch.matmul(fb_modulated_output.transpose(1,0),s_out)/s_out.shape[0]\n",
    "                p.grad = hebb_factor\n",
    "        \n",
    "    return model\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensorimotor(\n",
      "  (sensory_pop): SensoryPopulation(\n",
      "    (sensory_neurons_1): SensoryNeurons(\n",
      "      (linear): Linear(in_features=1, out_features=80, bias=True)\n",
      "      (resp_func): Sigmoid()\n",
      "    )\n",
      "    (sensory_neurons_2): SensoryNeurons(\n",
      "      (linear): Linear(in_features=1, out_features=20, bias=True)\n",
      "      (resp_func): Sigmoid()\n",
      "    )\n",
      "  )\n",
      "  (readout): Readout(\n",
      "    (readout_layer): Linear(in_features=100, out_features=1, bias=True)\n",
      "  )\n",
      "  (sig): Sigmoid()\n",
      ")\n",
      "Parameter containing:\n",
      "tensor([[0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099,\n",
      "         0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099,\n",
      "         0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099,\n",
      "         0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099,\n",
      "         0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099,\n",
      "         0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099,\n",
      "         0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099,\n",
      "         0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099,\n",
      "         0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099,\n",
      "         0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099,\n",
      "         0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099,\n",
      "         0.0099]])\n",
      "\n",
      "===========Check Grad============\n",
      "sensory_pop.sensory_neurons_1.linear.weight False\n",
      "sensory_pop.sensory_neurons_1.linear.bias False\n",
      "sensory_pop.sensory_neurons_2.linear.weight False\n",
      "sensory_pop.sensory_neurons_2.linear.bias False\n",
      "readout.readout_layer.weight False\n",
      "readout.readout_layer.bias True\n",
      "=================================\n",
      "\n",
      "bias = Parameter containing:\n",
      "tensor([1.], requires_grad=True)\n",
      "bias = Parameter containing:\n",
      "tensor([1.], requires_grad=True) loss = 1.0655721426010132\n",
      "Confusion Matrix: (target in columns)\n",
      "[[   0.    0.]\n",
      " [1000. 1000.]]\n",
      "bias = Parameter containing:\n",
      "tensor([0.8479], requires_grad=True) loss = 1.021173119544983\n",
      "bias = Parameter containing:\n",
      "tensor([0.7082], requires_grad=True) loss = 0.9838467240333557\n",
      "bias = Parameter containing:\n",
      "tensor([0.5809], requires_grad=True) loss = 0.9529162645339966\n",
      "bias = Parameter containing:\n",
      "tensor([0.4655], requires_grad=True) loss = 0.9276149272918701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/DeepMouse/lib/python3.7/site-packages/torch/nn/modules/loss.py:529: UserWarning: Using a target size (torch.Size([2000])) that is different to the input size (torch.Size([2000, 1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bias = Parameter containing:\n",
      "tensor([0.3616], requires_grad=True) loss = 0.9071518182754517\n",
      "bias = Parameter containing:\n",
      "tensor([0.2686], requires_grad=True) loss = 0.8907604217529297\n",
      "bias = Parameter containing:\n",
      "tensor([0.1856], requires_grad=True) loss = 0.877737283706665\n",
      "bias = Parameter containing:\n",
      "tensor([0.1117], requires_grad=True) loss = 0.8674588203430176\n",
      "bias = Parameter containing:\n",
      "tensor([0.0463], requires_grad=True) loss = 0.8593902587890625\n",
      "bias = Parameter containing:\n",
      "tensor([-0.0116], requires_grad=True) loss = 0.8530840873718262\n",
      "bias = Parameter containing:\n",
      "tensor([-0.0626], requires_grad=True) loss = 0.8481727838516235\n",
      "bias = Parameter containing:\n",
      "tensor([-0.1077], requires_grad=True) loss = 0.844357967376709\n",
      "bias = Parameter containing:\n",
      "tensor([-0.1473], requires_grad=True) loss = 0.8414011001586914\n",
      "bias = Parameter containing:\n",
      "tensor([-0.1822], requires_grad=True) loss = 0.8391134142875671\n",
      "bias = Parameter containing:\n",
      "tensor([-0.2129], requires_grad=True) loss = 0.8373457193374634\n",
      "bias = Parameter containing:\n",
      "tensor([-0.2399], requires_grad=True) loss = 0.8359807133674622\n",
      "bias = Parameter containing:\n",
      "tensor([-0.2635], requires_grad=True) loss = 0.8349282741546631\n",
      "bias = Parameter containing:\n",
      "tensor([-0.2843], requires_grad=True) loss = 0.8341167569160461\n",
      "bias = Parameter containing:\n",
      "tensor([-0.3026], requires_grad=True) loss = 0.8334916234016418\n",
      "bias = Parameter containing:\n",
      "tensor([-0.3186], requires_grad=True) loss = 0.8330097794532776\n",
      "bias = Parameter containing:\n",
      "tensor([-0.3326], requires_grad=True) loss = 0.8326391577720642\n",
      "bias = Parameter containing:\n",
      "tensor([-0.3450], requires_grad=True) loss = 0.8323536515235901\n",
      "bias = Parameter containing:\n",
      "tensor([-0.3558], requires_grad=True) loss = 0.8321338295936584\n",
      "bias = Parameter containing:\n",
      "tensor([-0.3653], requires_grad=True) loss = 0.8319646716117859\n",
      "bias = Parameter containing:\n",
      "tensor([-0.3736], requires_grad=True) loss = 0.8318346738815308\n",
      "bias = Parameter containing:\n",
      "tensor([-0.3809], requires_grad=True) loss = 0.8317344188690186\n",
      "bias = Parameter containing:\n",
      "tensor([-0.3873], requires_grad=True) loss = 0.8316575288772583\n",
      "bias = Parameter containing:\n",
      "tensor([-0.3929], requires_grad=True) loss = 0.8315982222557068\n",
      "bias = Parameter containing:\n",
      "tensor([-0.3979], requires_grad=True) loss = 0.8315525650978088\n",
      "bias = Parameter containing:\n",
      "tensor([-0.4022], requires_grad=True) loss = 0.8315174579620361\n",
      "bias = Parameter containing:\n",
      "tensor([-0.4060], requires_grad=True) loss = 0.8314904570579529\n",
      "bias = Parameter containing:\n",
      "tensor([-0.4093], requires_grad=True) loss = 0.8314695954322815\n",
      "bias = Parameter containing:\n",
      "tensor([-0.4122], requires_grad=True) loss = 0.831453800201416\n",
      "bias = Parameter containing:\n",
      "tensor([-0.4148], requires_grad=True) loss = 0.8314415216445923\n",
      "bias = Parameter containing:\n",
      "tensor([-0.4170], requires_grad=True) loss = 0.8314319252967834\n",
      "bias = Parameter containing:\n",
      "tensor([-0.4190], requires_grad=True) loss = 0.8314246535301208\n",
      "bias = Parameter containing:\n",
      "tensor([-0.4207], requires_grad=True) loss = 0.8314191102981567\n",
      "bias = Parameter containing:\n",
      "tensor([-0.4222], requires_grad=True) loss = 0.8314148783683777\n",
      "bias = Parameter containing:\n",
      "tensor([-0.4236], requires_grad=True) loss = 0.8314113020896912\n",
      "bias = Parameter containing:\n",
      "tensor([-0.4247], requires_grad=True) loss = 0.8314089179039001\n",
      "bias = Parameter containing:\n",
      "tensor([-0.4258], requires_grad=True) loss = 0.8314069509506226\n",
      "bias = Parameter containing:\n",
      "tensor([-0.4267], requires_grad=True) loss = 0.8314053416252136\n",
      "bias = Parameter containing:\n",
      "tensor([-0.4274], requires_grad=True) loss = 0.8314042091369629\n",
      "bias = Parameter containing:\n",
      "tensor([-0.4281], requires_grad=True) loss = 0.8314034342765808\n",
      "bias = Parameter containing:\n",
      "tensor([-0.4287], requires_grad=True) loss = 0.831402599811554\n",
      "bias = Parameter containing:\n",
      "tensor([-0.4293], requires_grad=True) loss = 0.8314021229743958\n",
      "bias = Parameter containing:\n",
      "tensor([-0.4297], requires_grad=True) loss = 0.8314019441604614\n",
      "bias = Parameter containing:\n",
      "tensor([-0.4301], requires_grad=True) loss = 0.8314014673233032\n",
      "bias = Parameter containing:\n",
      "tensor([-0.4305], requires_grad=True) loss = 0.8314012885093689\n",
      "bias = Parameter containing:\n",
      "tensor([-0.4308], requires_grad=True) loss = 0.8314008712768555\n",
      "bias = Parameter containing:\n",
      "tensor([-0.4311], requires_grad=True) loss = 0.8314008712768555\n",
      "bias = Parameter containing:\n",
      "tensor([-0.4313], requires_grad=True) loss = 0.8314006924629211\n",
      "bias = Parameter containing:\n",
      "tensor([-0.4315], requires_grad=True) loss = 0.8314007520675659\n",
      "bias = Parameter containing:\n",
      "tensor([-0.4317], requires_grad=True) loss = 0.8314005732536316\n",
      "bias = Parameter containing:\n",
      "tensor([-0.4319], requires_grad=True) loss = 0.8314006924629211\n",
      "bias = Parameter containing:\n",
      "tensor([-0.4320], requires_grad=True) loss = 0.8314005136489868\n",
      "bias = Parameter containing:\n",
      "tensor([-0.4322], requires_grad=True) loss = 0.8314003944396973\n",
      "bias = Parameter containing:\n",
      "tensor([-0.4323], requires_grad=True) loss = 0.8314006328582764\n",
      "bias = Parameter containing:\n",
      "tensor([-0.4324], requires_grad=True) loss = 0.8314005136489868\n",
      "bias = Parameter containing:\n",
      "tensor([-0.4324], requires_grad=True) loss = 0.8314005136489868\n",
      "bias = Parameter containing:\n",
      "tensor([-0.4325], requires_grad=True) loss = 0.8314003944396973\n",
      "bias = Parameter containing:\n",
      "tensor([-0.4326], requires_grad=True) loss = 0.831400454044342\n",
      "bias = Parameter containing:\n",
      "tensor([-0.4326], requires_grad=True) loss = 0.831400454044342\n",
      "bias = Parameter containing:\n",
      "tensor([-0.4327], requires_grad=True) loss = 0.8314003348350525\n",
      "bias = Parameter containing:\n",
      "tensor([-0.4327], requires_grad=True) loss = 0.831400454044342\n",
      "bias = Parameter containing:\n",
      "tensor([-0.4328], requires_grad=True) loss = 0.831400454044342\n",
      "bias = Parameter containing:\n",
      "tensor([-0.4328], requires_grad=True) loss = 0.8314003348350525\n",
      "bias = Parameter containing:\n",
      "tensor([-0.4328], requires_grad=True) loss = 0.8314002752304077\n",
      "bias = Parameter containing:\n",
      "tensor([-0.4329], requires_grad=True) loss = 0.831400454044342\n",
      "bias = Parameter containing:\n",
      "tensor([-0.4329], requires_grad=True) loss = 0.831400454044342\n",
      "bias = Parameter containing:\n",
      "tensor([-0.4329], requires_grad=True) loss = 0.8314003944396973\n",
      "bias = Parameter containing:\n",
      "tensor([-0.4329], requires_grad=True) loss = 0.831400454044342\n",
      "bias = Parameter containing:\n",
      "tensor([-0.4329], requires_grad=True) loss = 0.8314005136489868\n",
      "bias = Parameter containing:\n",
      "tensor([-0.4330], requires_grad=True) loss = 0.8314003348350525\n",
      "bias = Parameter containing:\n",
      "tensor([-0.4330], requires_grad=True) loss = 0.8314003348350525\n",
      "bias = Parameter containing:\n",
      "tensor([-0.4330], requires_grad=True) loss = 0.831400454044342\n",
      "bias = Parameter containing:\n",
      "tensor([-0.4330], requires_grad=True) loss = 0.8314002752304077\n",
      "bias = Parameter containing:\n",
      "tensor([-0.4330], requires_grad=True) loss = 0.831400454044342\n",
      "bias = Parameter containing:\n",
      "tensor([-0.4330], requires_grad=True) loss = 0.8314006924629211\n",
      "bias = Parameter containing:\n",
      "tensor([-0.4330], requires_grad=True) loss = 0.8314003944396973\n",
      "bias = Parameter containing:\n",
      "tensor([-0.4330], requires_grad=True) loss = 0.831400454044342\n",
      "bias = Parameter containing:\n",
      "tensor([-0.4330], requires_grad=True) loss = 0.8314005732536316\n",
      "bias = Parameter containing:\n",
      "tensor([-0.4330], requires_grad=True) loss = 0.8314003944396973\n",
      "bias = Parameter containing:\n",
      "tensor([-0.4330], requires_grad=True) loss = 0.831400454044342\n",
      "bias = Parameter containing:\n",
      "tensor([-0.4330], requires_grad=True) loss = 0.831400454044342\n",
      "bias = Parameter containing:\n",
      "tensor([-0.4330], requires_grad=True) loss = 0.8314005136489868\n",
      "bias = Parameter containing:\n",
      "tensor([-0.4330], requires_grad=True) loss = 0.8314003944396973\n",
      "bias = Parameter containing:\n",
      "tensor([-0.4330], requires_grad=True) loss = 0.831400454044342\n",
      "bias = Parameter containing:\n",
      "tensor([-0.4330], requires_grad=True) loss = 0.831400454044342\n",
      "bias = Parameter containing:\n",
      "tensor([-0.4330], requires_grad=True) loss = 0.8314003944396973\n",
      "bias = Parameter containing:\n",
      "tensor([-0.4330], requires_grad=True) loss = 0.8314002752304077\n",
      "bias = Parameter containing:\n",
      "tensor([-0.4330], requires_grad=True) loss = 0.8314005136489868\n",
      "bias = Parameter containing:\n",
      "tensor([-0.4330], requires_grad=True) loss = 0.8314002752304077\n",
      "bias = Parameter containing:\n",
      "tensor([-0.4330], requires_grad=True) loss = 0.8314005136489868\n",
      "bias = Parameter containing:\n",
      "tensor([-0.4330], requires_grad=True) loss = 0.8314005136489868\n",
      "bias = Parameter containing:\n",
      "tensor([-0.4330], requires_grad=True) loss = 0.8314002752304077\n",
      "bias = Parameter containing:\n",
      "tensor([-0.4330], requires_grad=True) loss = 0.831400454044342\n",
      "bias = Parameter containing:\n",
      "tensor([-0.4330], requires_grad=True) loss = 0.831400454044342\n",
      "bias = Parameter containing:\n",
      "tensor([-0.4330], requires_grad=True) loss = 0.831400454044342\n",
      "Confusion Matrix: (target in columns)\n",
      "[[   0. 1000.]\n",
      " [1000.    0.]]\n",
      "Parameter containing:\n",
      "tensor([[0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099,\n",
      "         0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099,\n",
      "         0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099,\n",
      "         0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099,\n",
      "         0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099,\n",
      "         0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099,\n",
      "         0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099,\n",
      "         0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099,\n",
      "         0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099,\n",
      "         0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099,\n",
      "         0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099,\n",
      "         0.0099]])\n",
      "bias = Parameter containing:\n",
      "tensor([-0.4330], requires_grad=True)\n",
      "Final train loss = 0.831400454044342,    valid loss = 0.8313257694244385 \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "lr = .5\n",
    "batch_size = 2000\n",
    "\n",
    "loss, loss_valid, model, y0, y1 = main(num_epochs = num_epochs, lr = lr, batch_size = batch_size, learning_rule = 'backprop')\n",
    "print(f'Final train loss = {loss[-1]},    valid loss = {loss_valid[-1]} \\n')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(2, 1, 1)\n",
    "ax.plot(np.arange(0,num_epochs),loss)\n",
    "plt.plot(np.arange(0,num_epochs),loss_valid)\n",
    "ax.set_xscale('log')\n",
    "# ax.set_yscale('log')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(y0,alpha=0.5)\n",
    "plt.hist(y1,alpha=0.5)\n",
    "plt.show()\n",
    "\n",
    "# plt.scatter(y0,y1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "lr = 1e-4\n",
    "batch_size = 100\n",
    "\n",
    "model.readout.readout_layer.bias.requires_grad = False\n",
    "loss, loss_valid, model, y0, y1 = main(num_epochs = num_epochs, lr = lr, batch_size = batch_size, learning_rule = 'global_gain', model = model)\n",
    "print(f'Final train loss = {loss[-1]},    valid loss = {loss_valid[-1]} \\n')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(2, 1, 1)\n",
    "ax.plot(np.arange(0,num_epochs),loss)\n",
    "\n",
    "plt.plot(np.arange(0,num_epochs),loss_valid)\n",
    "ax.set_xscale('log')\n",
    "# ax.set_yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# plt.imshow(model.readout.readout_layer.weight.detach())\n",
    "\n",
    "# plt.scatter(torch.cat((model.sensory_pop.sensory_neurons_1.linear.weight,model.sensory_pop.sensory_neurons_2.linear.weight),dim=0).detach(),\n",
    "#             model.readout.readout_layer.weight[0,:].detach())\n",
    "print(model.readout.readout_layer.bias)\n",
    "\n",
    "\n",
    "plt.scatter(model.sensory_pop.sensory_neurons_1.linear.weight.detach(),\n",
    "            model.readout.readout_layer.weight[0,0:10].detach(),color='r')\n",
    "plt.scatter(model.sensory_pop.sensory_neurons_2.linear.weight.detach(),\n",
    "            model.readout.readout_layer.weight[0,10:100].detach(),color='b')\n",
    "plt.ylabel('readout weight',fontsize=15)\n",
    "plt.xlabel('sigmoid weight',fontsize=15)\n",
    "plt.grid('both')\n",
    "plt.savefig('/Users/shahab/Desktop/Figs/VPL/sigW_vs_readW.png')\n",
    "plt.show()\n",
    "# plt.scatter(torch.cat((model.sensory_pop.sensory_neurons_1.linear.bias,model.sensory_pop.sensory_neurons_2.linear.bias),dim=0).detach(),\n",
    "#             model.readout.readout_layer.weight[0,:].detach())\n",
    "plt.scatter(model.sensory_pop.sensory_neurons_1.linear.bias.detach(),\n",
    "            model.readout.readout_layer.weight[0,0:10].detach(),color='r')\n",
    "plt.scatter(model.sensory_pop.sensory_neurons_2.linear.bias.detach(),\n",
    "            model.readout.readout_layer.weight[0,10:100].detach(),color='b')\n",
    "plt.ylabel('readout weight',fontsize=15)\n",
    "plt.xlabel('sigmoid bias',fontsize=15)\n",
    "plt.grid('both')\n",
    "plt.savefig('/Users/shahab/Desktop/Figs/VPL/sigB_vs_readW.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr = 1e-4, coh = 0.04-0.06\n",
    "pre_bias_1 = np.array([-433/1000, -425/1000, -435/1000, 0/1000, -384/1000])\n",
    "post_bias_1 = np.array([0/1000, 66/1000, 521/1000, 371/1000, 0/1000])\n",
    "\n",
    "# lr = 1e-4, coh = 0.02-0.04\n",
    "pre_bias_3 = np.array([-1000/1000,-1000/1000, -1000/1000])\n",
    "post_bias_3 = np.array([1000/1000, 919/1000,983/1000, ])\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.subplot()\n",
    "plt.bar(1,pre_bias_1.mean(),alpha=0.5,width=0.3,color='b')\n",
    "plt.errorbar(1,pre_bias_1.mean(),xerr=0,yerr=pre_bias_1.std(),marker='o',color='b')\n",
    "plt.bar(2,post_bias_1.mean(),alpha=0.5,width=0.3,color='b')\n",
    "plt.errorbar(2,post_bias_1.mean(),xerr=0,yerr=post_bias_1.std(),marker='o',color='b')\n",
    "plt.xticks([1,2],('pre','post'),fontsize=20)\n",
    "plt.ylabel('bias',fontsize = 20)\n",
    "plt.show()\n",
    "\n",
    "plt.bar(1,pre_bias_2.mean(),alpha=0.5,width=0.3,color='b')\n",
    "plt.errorbar(1,pre_bias_2.mean(),xerr=0,yerr=pre_bias_2.std(),marker='o',color='b')\n",
    "plt.bar(2,post_bias_2.mean(),alpha=0.5,width=0.3,color='b')\n",
    "plt.errorbar(2,post_bias_2.mean(),xerr=0,yerr=post_bias_2.std(),marker='o',color='b')\n",
    "plt.xticks([1,2],('pre','post'),fontsize=2)\n",
    "plt.ylabel('bias',fontsize = 20)\n",
    "plt.show()\n",
    "\n",
    "plt.bar(1,pre_bias_3.mean(),alpha=0.5,width=0.3,color='b')\n",
    "plt.errorbar(1,pre_bias_3.mean(),xerr=0,yerr=pre_bias_3.std(),marker='o',color='b')\n",
    "plt.bar(2,post_bias_3.mean(),alpha=0.5,width=0.3,color='b')\n",
    "plt.errorbar(2,post_bias_3.mean(),xerr=0,yerr=post_bias_3.std(),marker='o',color='b')\n",
    "plt.xticks([1,2],('pre','post'),fontsize=2)\n",
    "plt.ylabel('bias',fontsize = 20)\n",
    "plt.show()\n",
    "\n",
    "plt.bar(1,pre_bias_4.mean(),alpha=0.5,width=0.3,color='b')\n",
    "plt.errorbar(1,pre_bias_4.mean(),xerr=0,yerr=pre_bias_4.std(),marker='o',color='b')\n",
    "plt.bar(2,post_bias_4.mean(),alpha=0.5,width=0.3,color='b')\n",
    "plt.errorbar(2,post_bias_4.mean(),xerr=0,yerr=post_bias_4.std(),marker='o',color='b')\n",
    "plt.xticks([1,2],('pre','post'),fontsize=2)\n",
    "plt.ylabel('bias',fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
