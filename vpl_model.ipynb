{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "class SensoryNeurons(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_neurons, in_dim = 1, plastic = True, params = (5, -2.5)):\n",
    "        super(SensoryNeurons, self).__init__()\n",
    "        \n",
    "        self.in_dim = in_dim\n",
    "        self.plastic = plastic\n",
    "        self.params = params\n",
    "        self.num_neurons = num_neurons\n",
    "\n",
    "        self.linear = nn.Linear(self.in_dim,self.num_neurons, bias = True)\n",
    "        self.resp_func = nn.Sigmoid()\n",
    "\n",
    "        if not self.plastic:\n",
    "            self.set_linear_weights()\n",
    "            self.linear.weight.requires_grad = False\n",
    "        \n",
    "    def set_linear_weights(self):\n",
    "        \n",
    "        self.linear.weight = torch.nn.Parameter(data = self.params[0] + 0.2*torch.randn(self.num_neurons,1), requires_grad = False)\n",
    "        self.linear.bias = torch.nn.Parameter(data = self.params[1] + 0.2*torch.randn(self.num_neurons), requires_grad = False)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.linear(x)\n",
    "        x = self.resp_func(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "        \n",
    "        \n",
    "class SensoryPopulation(nn.Module):\n",
    "    def __init__(self, num_neurons, plastic = True, population_ratio = 0.5):\n",
    "        super(SensoryPopulation, self).__init__()\n",
    "        \n",
    "        self.num_neurons = num_neurons\n",
    "        self.plastic = plastic\n",
    "        self.num_neurons_group1 = round(self.num_neurons * population_ratio)\n",
    "        self.num_neurons_group2 = round(self.num_neurons * (1.0 - population_ratio))\n",
    "        self.sensory_neurons_1 = SensoryNeurons(num_neurons = self.num_neurons_group1, in_dim = 1, plastic = self.plastic, params = (5, -2.5))\n",
    "        self.sensory_neurons_2 = SensoryNeurons(num_neurons = self.num_neurons_group2, in_dim = 1, plastic = self.plastic, params = (-5, -2.5))\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        x1 = self.sensory_neurons_1(x)\n",
    "        x2 = self.sensory_neurons_2(x)\n",
    "        \n",
    "        out = torch.cat((x1, x2),dim = 1)       \n",
    "        \n",
    "        return out\n",
    "            \n",
    "        \n",
    "    \n",
    "class Readout(nn.Module):\n",
    "    def __init__(self,num_classes = 2, in_dim = 10):\n",
    "        super(Readout, self).__init__()\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        self.in_dim = in_dim\n",
    "        \n",
    "        self.readout_layer = nn.Linear(self.in_dim, self.num_classes, bias = True)\n",
    "        self._set_weights()\n",
    "        \n",
    "    def _set_weights(self):\n",
    "        self.readout_layer.weight = nn.Parameter(torch.ones(self.readout_layer.weight.shape),requires_grad=False) #\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        return (self.readout_layer(x))\n",
    "    \n",
    "    \n",
    "class Sensorimotor(nn.Module):\n",
    "    def __init__(self, num_sensory_neurons = 10, sensory_plastic = True, sensory_pop_ratio = 0.5, num_classes = 2):\n",
    "        super(Sensorimotor, self).__init__()\n",
    "        \n",
    "        self.sensory_pop = SensoryPopulation(num_neurons = num_sensory_neurons, plastic = sensory_plastic, population_ratio = sensory_pop_ratio)\n",
    "        self.readout = Readout(num_classes = num_classes, in_dim = num_sensory_neurons)\n",
    "        self.sig = nn.Sigmoid()\n",
    "#         self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        sensory_out = self.sensory_pop(x)\n",
    "        readout_out = self.readout(sensory_out)\n",
    "        y = self.sig(readout_out)\n",
    "#         y = self.softmax(readout_out)\n",
    "#         y = readout_out\n",
    "        \n",
    "        return y, readout_out\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "\n",
    "class Stimulus(data.DataLoader):\n",
    "    def __init__(self, min_coherence = 0.8, max_coherence = 1):\n",
    "        \n",
    "        self.min_coherence = min_coherence\n",
    "        self.max_coherence = max_coherence\n",
    "        \n",
    "        self.NUM_SAMPLPES_PER_CATEGORY = 1000\n",
    "        \n",
    "        data1 = torch.rand(self.NUM_SAMPLPES_PER_CATEGORY)*(self.max_coherence - self.min_coherence) + self.min_coherence\n",
    "        data2 = -(torch.rand(self.NUM_SAMPLPES_PER_CATEGORY)*(self.max_coherence - self.min_coherence) + self.min_coherence)\n",
    "#         target1 = torch.zeros(data1.shape, dtype = int)\n",
    "#         target2 = torch.ones(data2.shape, dtype = int)\n",
    "        target1 = torch.zeros(data1.shape)\n",
    "        target2 = torch.ones(data2.shape)\n",
    "        \n",
    "        self.data = torch.cat((data1, data2), dim = 0).unsqueeze(0).t()\n",
    "        self.target = torch.cat((target1, target2), dim = 0).unsqueeze(0).t()\n",
    " \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        \n",
    "        return (self.data[index], self.target[index])\n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.data)\n",
    "    \n",
    "     \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class ConfusionMeter(object):\n",
    "    '''compute and show confusion matrix'''\n",
    "    def __init__(self, num_class):\n",
    "        self.num_class = num_class\n",
    "        self.mat = np.zeros((num_class, num_class))\n",
    "        self.precision = []\n",
    "        self.recall = []\n",
    "\n",
    "    def update(self, pred, tar):\n",
    "        pred, tar = pred.cpu().numpy(), tar.cpu().numpy()\n",
    "        pred = np.squeeze(pred)\n",
    "        tar = np.squeeze(tar)\n",
    "        for p,t in zip(pred.flat, tar.flat):\n",
    "            self.mat[p][t] += 1\n",
    "\n",
    "    def print_mat(self):\n",
    "        print('Confusion Matrix: (target in columns)')\n",
    "        print(self.mat)\n",
    "\n",
    "    def plot_mat(self, path, dictionary=None, annotate=False):\n",
    "        plt.figure(dpi=600)\n",
    "        plt.imshow(self.mat,\n",
    "            cmap=plt.cm.jet,\n",
    "            interpolation=None,\n",
    "            extent=(0.5, np.shape(self.mat)[0]+0.5, np.shape(self.mat)[1]+0.5, 0.5))\n",
    "        width, height = self.mat.shape\n",
    "        if annotate:\n",
    "            for x in range(width):\n",
    "                for y in range(height):\n",
    "                    plt.annotate(str(int(self.mat[x][y])), xy=(y+1, x+1),\n",
    "                                 horizontalalignment='center',\n",
    "                                 verticalalignment='center',\n",
    "                                 fontsize=8)\n",
    "\n",
    "        if dictionary is not None:\n",
    "            plt.xticks([i+1 for i in range(width)],\n",
    "                       [dictionary[i] for i in range(width)],\n",
    "                       rotation='vertical')\n",
    "            plt.yticks([i+1 for i in range(height)],\n",
    "                       [dictionary[i] for i in range(height)])\n",
    "        plt.xlabel('Ground Truth')\n",
    "        plt.ylabel('Prediction')\n",
    "        plt.colorbar()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(path, format='svg')\n",
    "        plt.clf()\n",
    "\n",
    "        # for i in range(width):\n",
    "        #     if np.sum(self.mat[i,:]) != 0:\n",
    "        #         self.precision.append(self.mat[i,i] / np.sum(self.mat[i,:]))\n",
    "        #     if np.sum(self.mat[:,i]) != 0:\n",
    "        #         self.recall.append(self.mat[i,i] / np.sum(self.mat[:,i]))\n",
    "        # print('Average Precision: %0.4f' % np.mean(self.precision))\n",
    "        # print('Average Recall: %0.4f' % np.mean(self.recall))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "import torch.optim as optim\n",
    "\n",
    "def main(num_epochs = 1000, lr = 1e-1, batch_size = 100, learning_rule = 'backprop', model = None): # learning_rule can be 'backprop' or 'global_gain'\n",
    "    \n",
    "    if model is None:\n",
    "        model = Sensorimotor(num_sensory_neurons = 100, \n",
    "                             sensory_plastic = False, \n",
    "                             sensory_pop_ratio = 0.5,\n",
    "                             num_classes = 1)\n",
    "    \n",
    "    print(model)\n",
    "    print(model.readout.readout_layer.weight)\n",
    "    print('\\n===========Check Grad============')\n",
    "    for name, param in model.named_parameters():\n",
    "        print(name, param.requires_grad)\n",
    "    print('=================================\\n')    \n",
    "    \n",
    "    params = model.parameters()\n",
    "    \n",
    "    \n",
    "    optimizer = optim.SGD(params, lr=lr, momentum=0, dampening=0, weight_decay=0, nesterov=False)\n",
    "#     loss = nn.CrossEntropyLoss()\n",
    "#     loss = nn.NLLLoss()\n",
    "#     loss = nn.MSELoss()\n",
    "    loss = nn.BCELoss(reduction='mean')\n",
    "    \n",
    "    dataset_train = Stimulus(min_coherence = 0.8, max_coherence = 1) # set the range of coherences\n",
    "    dataset_valid = Stimulus(min_coherence = 0.8, max_coherence = 1)\n",
    "#     dataset_valid = Stimulus(min_coherence = 0.04, max_coherence = 0.06)\n",
    "\n",
    "\n",
    "    sampler_train = data.RandomSampler(dataset_train)\n",
    "    train_dl = data.DataLoader(dataset_train,\n",
    "                             batch_size=batch_size,\n",
    "                             sampler=sampler_train,\n",
    "                             shuffle=False,\n",
    "                             num_workers=0,\n",
    "                             pin_memory=True,\n",
    "                             drop_last=True)\n",
    "    \n",
    "    sampler_valid = data.RandomSampler(dataset_valid)\n",
    "    valid_dl = data.DataLoader(dataset_valid,\n",
    "                             batch_size=batch_size,\n",
    "                             sampler=sampler_valid,\n",
    "                             shuffle=False,\n",
    "                             num_workers=0,\n",
    "                             pin_memory=True,\n",
    "                             drop_last=True)\n",
    "    \n",
    "\n",
    "\n",
    "    all_loss = []\n",
    "    all_loss_valid = []\n",
    "    \n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        Loss = 0\n",
    "        \n",
    "        for stimulus, target in train_dl:\n",
    "            \n",
    "            decision, _ = model(stimulus)\n",
    "            L = loss(decision, target.squeeze())\n",
    "#             print(f'dec={decision}, t={target.squeeze()}, loss={L}')\n",
    "            Loss += L/len(train_dl)\n",
    "            optimizer.zero_grad()\n",
    "            L.backward()      \n",
    "            \n",
    "            if learning_rule != 'backprop':\n",
    "                model = personalized_backward(model, stimulus, target, method=learning_rule)\n",
    "\n",
    "            \n",
    "            optimizer.step()\n",
    "            del L\n",
    "            \n",
    "            \n",
    "        Loss_valid = 0\n",
    "        conf_mat = ConfusionMeter(num_class=2)\n",
    "        i = 0\n",
    "        y1 = []\n",
    "        y0 = []\n",
    "        for stimulus, target in valid_dl:\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                decision, readout = model(stimulus)\n",
    "                \n",
    "                pred = decision.clone()\n",
    "\n",
    "                \n",
    "                decision_copy = decision.clone()\n",
    "                pred = target.clone()\n",
    "#                 y1.extend(readout[:,1].detach())\n",
    "#                 y0.extend(readout[:,0].detach())\n",
    "#                 idx1 = decision_copy[:,1] < decision_copy[:,0]\n",
    "#                 idx2 = decision_copy[:,1] >= decision_copy[:,0]\n",
    "                idx1 = decision_copy < 0.5\n",
    "                idx2 = decision_copy >= 0.5\n",
    "                pred[idx1] = 0\n",
    "                pred[idx2] = 1\n",
    "\n",
    "                target_copy = target.squeeze().int()\n",
    "                conf_mat.update(pred.int(),target_copy)\n",
    "                del pred\n",
    "                L = loss(decision, target.squeeze())\n",
    "                Loss_valid += L/len(train_dl)\n",
    "                \n",
    "#                 \n",
    "\n",
    "                del L\n",
    "        if epoch == 0:\n",
    "            conf_mat.print_mat()\n",
    "            \n",
    "        all_loss.append(Loss)\n",
    "        all_loss_valid.append(Loss_valid)\n",
    "        \n",
    "    conf_mat.print_mat()\n",
    "    print(model.readout.readout_layer.weight)\n",
    "\n",
    "\n",
    "        # print(f'epoch {epoch},   training Loss = {Loss},   validation Loss = {Loss_valid}')\n",
    "    return all_loss, all_loss_valid, model, y0, y1\n",
    "            \n",
    "            \n",
    "def personalized_backward(model, stimulus, target, method = 'global_gain'):\n",
    "    \n",
    "    if method == 'global_gain': \n",
    "        # only adds to the weight value:  if weight is negative (positive) adds a negative (positive) value \n",
    "        for n,p in model.named_parameters():\n",
    "\n",
    "            if p.requires_grad:\n",
    "#                 print(f'{n}, mean = {p.mean()}, std = {p.std()}')\n",
    "                p_sign = p.clone()\n",
    "                p_sign[p_sign>0] = 1\n",
    "                p_sign[p_sign<0] = -1\n",
    "                p.grad = -1*torch.ones((p.grad.shape),requires_grad=True)*p_sign\n",
    "    \n",
    "    if method == 'hebbian': \n",
    "        \n",
    "        for n,p in model.named_parameters():\n",
    "            \n",
    "            if ('readout' in n) and ('weight' in n):\n",
    "                output = model(stimulus)\n",
    "                s_out = model.sensory_pop(stimulus)\n",
    "                hebb_factor = torch.matmul(output.transpose(1,0),s_out)/s_out.shape[0]\n",
    "                p.grad = hebb_factor\n",
    "        \n",
    "    if method == 'hebbian_wfb':\n",
    "        \n",
    "        for n,p in model.named_parameters():\n",
    "            \n",
    "            if ('readout' in n) and ('weight' in n):\n",
    "                target = target.unsqueeze(0)\n",
    "                \n",
    "                # create feedback based on the target values\n",
    "                fb = torch.cat((target,(target-1).abs()),dim=0).squeeze().transpose(1,0)\n",
    "                \n",
    "                output = model(stimulus)\n",
    "                fb_modulated_output = output*fb\n",
    "                s_out = model.sensory_pop(stimulus)\n",
    "                hebb_factor = torch.matmul(fb_modulated_output.transpose(1,0),s_out)/s_out.shape[0]\n",
    "                p.grad = hebb_factor\n",
    "        \n",
    "    return model\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensorimotor(\n",
      "  (sensory_pop): SensoryPopulation(\n",
      "    (sensory_neurons_1): SensoryNeurons(\n",
      "      (linear): Linear(in_features=1, out_features=50, bias=True)\n",
      "      (resp_func): Sigmoid()\n",
      "    )\n",
      "    (sensory_neurons_2): SensoryNeurons(\n",
      "      (linear): Linear(in_features=1, out_features=50, bias=True)\n",
      "      (resp_func): Sigmoid()\n",
      "    )\n",
      "  )\n",
      "  (readout): Readout(\n",
      "    (readout_layer): Linear(in_features=100, out_features=1, bias=True)\n",
      "  )\n",
      "  (sig): Sigmoid()\n",
      ")\n",
      "Parameter containing:\n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "\n",
      "===========Check Grad============\n",
      "sensory_pop.sensory_neurons_1.linear.weight False\n",
      "sensory_pop.sensory_neurons_1.linear.bias False\n",
      "sensory_pop.sensory_neurons_2.linear.weight False\n",
      "sensory_pop.sensory_neurons_2.linear.bias False\n",
      "readout.readout_layer.weight False\n",
      "readout.readout_layer.bias True\n",
      "=================================\n",
      "\n",
      "Confusion Matrix: (target in columns)\n",
      "[[   0.    0.]\n",
      " [1000. 1000.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/DeepMouse/lib/python3.7/site-packages/torch/nn/modules/loss.py:529: UserWarning: Using a target size (torch.Size([2000])) that is different to the input size (torch.Size([2000, 1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: (target in columns)\n",
      "[[   0.    0.]\n",
      " [1000. 1000.]]\n",
      "Parameter containing:\n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "Final train loss = 50.0,    valid loss = 50.0 \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "lr = 1\n",
    "batch_size = 2000\n",
    "\n",
    "loss, loss_valid, model, y0, y1 = main(num_epochs = num_epochs, lr = lr, batch_size = batch_size, learning_rule = 'backprop')\n",
    "print(f'Final train loss = {loss[-1]},    valid loss = {loss_valid[-1]} \\n')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(2, 1, 1)\n",
    "ax.plot(np.arange(0,num_epochs),loss)\n",
    "plt.plot(np.arange(0,num_epochs),loss_valid)\n",
    "ax.set_xscale('log')\n",
    "# ax.set_yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(y0,alpha=0.5)\n",
    "plt.hist(y1,alpha=0.5)\n",
    "plt.show()\n",
    "\n",
    "# plt.scatter(y0,y1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "lr = 1e-4\n",
    "batch_size = 100\n",
    "\n",
    "model.readout.readout_layer.bias.requires_grad = False\n",
    "loss, loss_valid, model, y0, y1 = main(num_epochs = num_epochs, lr = lr, batch_size = batch_size, learning_rule = 'global_gain', model = model)\n",
    "print(f'Final train loss = {loss[-1]},    valid loss = {loss_valid[-1]} \\n')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(2, 1, 1)\n",
    "ax.plot(np.arange(0,num_epochs),loss)\n",
    "\n",
    "plt.plot(np.arange(0,num_epochs),loss_valid)\n",
    "ax.set_xscale('log')\n",
    "# ax.set_yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# plt.imshow(model.readout.readout_layer.weight.detach())\n",
    "\n",
    "# plt.scatter(torch.cat((model.sensory_pop.sensory_neurons_1.linear.weight,model.sensory_pop.sensory_neurons_2.linear.weight),dim=0).detach(),\n",
    "#             model.readout.readout_layer.weight[0,:].detach())\n",
    "print(model.readout.readout_layer.bias)\n",
    "\n",
    "\n",
    "plt.scatter(model.sensory_pop.sensory_neurons_1.linear.weight.detach(),\n",
    "            model.readout.readout_layer.weight[0,0:10].detach(),color='r')\n",
    "plt.scatter(model.sensory_pop.sensory_neurons_2.linear.weight.detach(),\n",
    "            model.readout.readout_layer.weight[0,10:100].detach(),color='b')\n",
    "plt.ylabel('readout weight',fontsize=15)\n",
    "plt.xlabel('sigmoid weight',fontsize=15)\n",
    "plt.grid('both')\n",
    "plt.savefig('/Users/shahab/Desktop/Figs/VPL/sigW_vs_readW.png')\n",
    "plt.show()\n",
    "# plt.scatter(torch.cat((model.sensory_pop.sensory_neurons_1.linear.bias,model.sensory_pop.sensory_neurons_2.linear.bias),dim=0).detach(),\n",
    "#             model.readout.readout_layer.weight[0,:].detach())\n",
    "plt.scatter(model.sensory_pop.sensory_neurons_1.linear.bias.detach(),\n",
    "            model.readout.readout_layer.weight[0,0:10].detach(),color='r')\n",
    "plt.scatter(model.sensory_pop.sensory_neurons_2.linear.bias.detach(),\n",
    "            model.readout.readout_layer.weight[0,10:100].detach(),color='b')\n",
    "plt.ylabel('readout weight',fontsize=15)\n",
    "plt.xlabel('sigmoid bias',fontsize=15)\n",
    "plt.grid('both')\n",
    "plt.savefig('/Users/shahab/Desktop/Figs/VPL/sigB_vs_readW.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr = 1e-4, coh = 0.04-0.06\n",
    "pre_bias_1 = np.array([-433/1000, -425/1000, -435/1000, 0/1000, -384/1000])\n",
    "post_bias_1 = np.array([0/1000, 66/1000, 521/1000, 371/1000, 0/1000])\n",
    "\n",
    "# lr = 1e-4, coh = 0.02-0.04\n",
    "pre_bias_3 = np.array([-1000/1000,-1000/1000, -1000/1000])\n",
    "post_bias_3 = np.array([1000/1000, 919/1000,983/1000, ])\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.subplot()\n",
    "plt.bar(1,pre_bias_1.mean(),alpha=0.5,width=0.3,color='b')\n",
    "plt.errorbar(1,pre_bias_1.mean(),xerr=0,yerr=pre_bias_1.std(),marker='o',color='b')\n",
    "plt.bar(2,post_bias_1.mean(),alpha=0.5,width=0.3,color='b')\n",
    "plt.errorbar(2,post_bias_1.mean(),xerr=0,yerr=post_bias_1.std(),marker='o',color='b')\n",
    "plt.xticks([1,2],('pre','post'),fontsize=20)\n",
    "plt.ylabel('bias',fontsize = 20)\n",
    "plt.show()\n",
    "\n",
    "plt.bar(1,pre_bias_2.mean(),alpha=0.5,width=0.3,color='b')\n",
    "plt.errorbar(1,pre_bias_2.mean(),xerr=0,yerr=pre_bias_2.std(),marker='o',color='b')\n",
    "plt.bar(2,post_bias_2.mean(),alpha=0.5,width=0.3,color='b')\n",
    "plt.errorbar(2,post_bias_2.mean(),xerr=0,yerr=post_bias_2.std(),marker='o',color='b')\n",
    "plt.xticks([1,2],('pre','post'),fontsize=2)\n",
    "plt.ylabel('bias',fontsize = 20)\n",
    "plt.show()\n",
    "\n",
    "plt.bar(1,pre_bias_3.mean(),alpha=0.5,width=0.3,color='b')\n",
    "plt.errorbar(1,pre_bias_3.mean(),xerr=0,yerr=pre_bias_3.std(),marker='o',color='b')\n",
    "plt.bar(2,post_bias_3.mean(),alpha=0.5,width=0.3,color='b')\n",
    "plt.errorbar(2,post_bias_3.mean(),xerr=0,yerr=post_bias_3.std(),marker='o',color='b')\n",
    "plt.xticks([1,2],('pre','post'),fontsize=2)\n",
    "plt.ylabel('bias',fontsize = 20)\n",
    "plt.show()\n",
    "\n",
    "plt.bar(1,pre_bias_4.mean(),alpha=0.5,width=0.3,color='b')\n",
    "plt.errorbar(1,pre_bias_4.mean(),xerr=0,yerr=pre_bias_4.std(),marker='o',color='b')\n",
    "plt.bar(2,post_bias_4.mean(),alpha=0.5,width=0.3,color='b')\n",
    "plt.errorbar(2,post_bias_4.mean(),xerr=0,yerr=post_bias_4.std(),marker='o',color='b')\n",
    "plt.xticks([1,2],('pre','post'),fontsize=2)\n",
    "plt.ylabel('bias',fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
